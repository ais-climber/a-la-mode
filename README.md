# Neural Semantics
A neuro-symbolic interface, intended for both **model extraction** (extracting knowledge from a net) as well as **model building** (building a net from a knowledge base).  The name comes from the core idea -- that the internal dynamics of neural networks can be used as formal semantics of knowledge bases.

## :heavy_check_mark: Supported Features:
- Model extraction
- Countermodel generation (via a random search, no construction yet)
- Modal and conditional reasoning
- Inferring what the net has learned before and after learning

## â— Current Limitations:
- Nets must be feed-forward, with a binary step activation function
- Nets currently must be hand-crafted
- Nets must be used for classification tasks in discrete domains
- Nets learn via (unsupervised) Hebbian learning
- Knowledge bases are expressed in a certain restricted modal syntax (see below)

## ðŸ“ Planned Features:
- Model building
- Counter-model building
- Proper sigmoid activation functions
- Plug-and-play with your existing Tensorflow model
- Tasks beyond classification
- Learning via backpropagation (!)
- Predicate/quantifier reasoning

# :brain: The Translation

| Syntax      | Neural Network  |
| ----------- | ------------------------------------------- |
| `K P`       | The neurons **reachable** from the set `P`  |
| `T P`       | **Forward-Propagation** of `P`              |
|  `P+ Q`     | Do **Hebbian Update** on `P`, then eval `Q` |

Conditionals are expressible in this language:  P â‡’ Q can be expressed as `TP ðŸ ’ Q`.

( so sentences in a knowledge base correspond to the dynamics of the net)

# ðŸ’» Running the Program


# :brain: Trying It Out
This program is currently in development, and many of the planned features involve significant research efforts.  So what the program can do right now is somewhat limited.  What you _can_ do with it is hand-craft a neural network and infer some things about what the net knows, expects, and learns.  (There is planned support for being able to plug-and-play with your own Tensorflow model.) 

To get you started, copy the following into a file within your `neural-semantics/` directory -- say `neural-semantics/penguin.py`.  Now navigate to this directory in your terminal and run `python3 penguin.py` (or `py penguin.py` for Windows).

```python
from BFNN import *
from Model import *

# An example that illustrates how Hebbian learning can learn
# a counterexample to a conditional while preserving the conditional.
# In this case, the net learns that penguins don't fly, while preserving
# the fact that typically birds *do* fly.
nodes = set(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
layers = [['a', 'b', 'c', 'd', 'e'], ['f', 'g'], ['h']]
weights = {('a', 'f'): 1.0, ('a', 'g'): 0.0, ('b', 'f'): 0.0, ('b', 'g'): -2.0, 
           ('c', 'f'): 0.0, ('c', 'g'): 3.0, ('d', 'f'): 0.0, ('d', 'g'): 3.0,
           ('e', 'f'): 0.0, ('e', 'g'): 3.0, ('f', 'h'): 2.0, ('g', 'h'): -2.0}
threshold = 0.0
rate = 1.0
prop_map = {'bird': {'a'}, 'penguin': {'a', 'b'}, 
            'orca': {'b', 'c'}, 'zebra': {'b', 'd'}, 
            'panda': {'b', 'e'}, 'flies': {'h'}}
net = BFNN(nodes, layers, weights, threshold, rate)
model = Model(net, prop_map)

print("> penguin â†’ bird \n    ", model.is_model("penguin â†’ bird"), "\n")
print("> bird â‡’ flies \n    ", model.is_model("bird â‡’ flies"), "\n")
print("> penguin â‡’ flies \n    ", model.is_model("penguin â‡’ flies"), "\n")
print("> orca+ zebra+ panda+\n>  (bird â‡’ flies) \n    ", model.is_model("orca+ (zebra+ (panda+ (bird â‡’ flies)))"), "\n")
print("> orca+ zebra+ panda+\n>  (penguin â‡’ flies) \n    ", model.is_model("orca+ (zebra+ (panda+ (penguin â‡’ flies)))"))
```


# ðŸ”— Links and Resources
For more details on what makes this neuro-symbolic interface work, see our [paper](https://journals.flvc.org/FLAIRS/article/download/130735/133901).

What drives our program is the idea that neural networks can be used as formal semantics of knowledge bases.  If you're interested in learning more, I highly recommend starting with:

- Leitgeb, Hannes. **Neural network models of conditionals: An introduction**. [[pdf]](https://www.academia.edu/download/32793110/LeitgebSanSebastianFINAL.pdf)
- A.S. dâ€™Avila Garcez,  K. Broda, D.M. Gabbay.  **Symbolic knowledge extraction from trained neural
networks: A sound approach**.  [[pdf]](https://www.sciencedirect.com/science/article/pii/S0004370200000771/pdf?md5=f782984da6f1244a563048b352a31ce5&pid=1-s2.0-S0004370200000771-main.pdf)
- Laura Giordano, Valentina Gliozzi, and Daniele Theseider DuprÃ©.  **A conditional, a fuzzy and a probabilistic interpretation
of self-organising maps**. [[pdf]](https://arxiv.org/pdf/2103.06854.pdf)
